---
layout: post
title: NBA 2019 Season- Daily Fantasy
---

My results and expected long term success in playing daily fantasy during the 2018-2019 NBA Season

# Overview

With yet another season of the NBA, I once again tried to compete in daily fantasy competitions using a statistical model.  Similar to last year, I focused on refining my models and algorithms and on figuring out whether entering these contests can be profitable.  The big changes from last year included:

1. Model Improvements: Testing models developed using xgboost with new features

2. Better Benchmarking: Assessing my models on estimated win percentage instead of expected fantasy points

3. New Contests: Entering contests at Draftkings in addition to the contests on FanDuel

# Model improvements

This season, the models predicting fantasy points improved dramatically, most likely due to the use of xgboost.  Comparing the best model from this season to the best model from last season, there's a large improvement:


| Model || MSE |
| Old Model || 102 |
| New Model || 67 |


These improvements most likely came from two sources: using xgboost over my old sklearn ridge regression and gbm models and incorporating new features.  Though it's hard to separate the impact of each of these improvements, I did observe a large isolated improvement from xgboost.  When using the same build sample and features, xgboost reduced the MSE from 85.1 to 64.6.  This improvement may even be understated: my old sklearn models were tuned pretty extensively, but these xgboost models were not.  If the xgboost model were to be tuned further, there could be further MSE improvements.

New and improved also likely improved the models this season.  Based on several iterative model builds, the biggest value-added feature appeared to be changing how I included team-level data.  Previously, I had dummy indicators for each team, but that had a few downsides.  Team performance wasn't consistent season over season (and probably not even within a season) and data was very sparse for each team in the league.  To solve these problems, I generalized team-level data by removing these dummy indicators and adding the average defensive and offensive ratings and the points scored by each team.  As a result, these features were more grounded than the previous variables and allowed me to use past season data to build models.  

I did try testing these other features and sampling methods, but observed limited or no performance improvements:
- per minute metrics
- building on a smaller subset based on minutes played per game
- reducing correlation of past performance metrics based on last n games

# Better Benchmarking

I also developed a better way to assess the business value of new models by tying their performance directly to the daily fantasy contests.  Assessing performance from the business value is important because there's a lot of non-linear behavior between the model's prediction and the outcome of a contest.  First, the model predictions are put into an algorithm to determine the optimal daily fantasy lineup.  Then this lineup competes against the lineups of others.  Hypothetically, there could exist a scenario where selecting a typically lower scoring and less popular player is more profitable due to the differentiation from the competition.  Because of those non-linear events, it's best to assess model performance from the last step, which in this case is whether the lineup won.  

Using actual daily fantasy contest data for 50/50 contests (where the top 50% of lineups win) enabled me to simulate the winning percentage of each model.  Interestingly, I noticed an unintuitive pattern where the highest fantasy point model was not the best winning model:

| Model || Mean FanDuel Fantasy Points || Winning Perc. |
| Model A || 292 ||	52% |
| Model C || 290 || 48% |
| Model D || 297 || 45% |
| FanDuel Model || 253 || 14% |

Among my models, there's an inverse relationship between mean fantasy points and winning percentages.  Intuitively, you'd expect that higher fantasy points would translate into higher winning percentages.  My hypothesis for this relationship is that the additional data present in Model D over Model A caused Model D to produce predictions more similar to the crowd, causing the models to lose some of their competitive edge.  Compared to Model A, Model D has new features related to the variance of fantasy points over the last n games.  I thought that it might make sense to penalize players that are not consistent and have high variance of their performance.  When users of FanDuel set their lineup, the site provides a line graph of a player's past n games, showing the trend and stability of their fantasy output.  I hypothesize that since this variance data is so easily available to other users, many users somehow incorporate variance when making their choices, which causes Model D to have more similar lineups with the competition.  When this variance data is not easily accessible, such as through DraftKing's website, I found that including this variance feature did indeed increase the winning percentage of the model (as shown in the section below).

It's also totally possible that this is just noise due to the low sample.  I had 42 contests, so losing an additional game would've resulted in a 2-3% drop in winning percentage.

I also found it interesting how poorly the default FanDuel model performed, winning only 14% of contests even when choosing the optimal lineup given these predictions.  This helps prove all the additional insights that the competition now uses when choosing their lineups.  

# New Contests

This season, I also competed on another daily fantasy website, DraftKings.  Similar to my first season on FanDuel, on DraftKings, I competed in the beginner 50/50 contests, where you win if you're in the top 50% of entries (which consist of inexperienced users at DraftKings).  Assuming that these users are truly beginners, these contests should be less competitive than the regular ones that are open to all users.

Despite this short-term perk of beginner contests, I also believe DraftKings is strategically better suited for models and lineup optimization due to the more complex scoring and lineup logic.  The scoring logic in DraftKings is much more complex than FanDuel, making it more difficult for a typical user to estimate a player's fantasy point output.  In DraftKings, bonus points are awarded for double-doubles and additional bonus points are awarded for triple doubles.  In addition, there are bonus points for 3 pointers.  Here's the full scoring logic:

- DraftKing Fantasy Points = Points + 0.5*3 Point Makes + 1.25*Rebounds + 1.5*Assists + 2*Blocks + 2*Steals -0.5*Turnovers + 1.5*Double-Double + 3*Triple-Double_

The lineups are also more complex than FanDuel.  In DraftKings, there are spots for each 5-spot position, but also spots for more generalized positions (i.e. guards, forwards, and utility).  Having these additional degrees of freedom increases the number of possible lineups, which increases the difficulty of finding the optimal lineup without an algorithm.

In my first season on DraftKings, I performed decent, but did not hit the 50% winning percentage:
| Model || Mean DraftKings Fantasy Points || Winning Perc. |
| Model DK A || 246 || 19% |
| Model DK D || 268 || 48% |

For DraftKings, my dataset was limited to just 21 games, so the variance in these metrics is relatively high and it's hard to get a read of the overall profitability of the best performing model, Model DK D.  There does, however, appear to be a substantial improvement from Model DK A to model DK D.  Similar to the FD versions of these models, Model DK D includes features related to the variance of past fantasy performance.  In this case, this feature appears to improve both the predictive power of the model and the competitiveness against other entries.  

# Takeaways and Next Steps

Though the progress this season didn't result in a massively profitable model, it did set up the infrastructure well for next season.  On the modeling side, I generalized the team feature, allowing me to train models on previous seasons, which should allow me to start entering contests sooner next season.  On the assessment side, I set up a process to more accurately tie my model performance to profit.  This should enable me to make better decisions on new models and features in the future.  Finally, I built the ability to enter contests on DraftKings, giving me another option for entering contests.

This work should allow me to be more competitive next season, but there are some risks with next season that may weaken the effectiveness of my models.

First, the general competition may embrace similar lineup optimization frameworks, chipping away at the effectiveness of my process.  I've seen several sites that give away free optimal lineups, and I've see FanDuel's website offer recommended lineups and picks that may use a similar framework.  If the general competition does embrace these tools, then my lineups will lose some of their competitive edge, making it harder to win contests.

Second, the legalization of sports betting may push the industry to develop better models, perhaps even making my models obsolete.  With more and more states legalizing sports betting, there are more incentives for individuals and businesses to develop their own models to predict sports events.  As these models improve, there may be some runover to the models that predict daily fantasy performance.  If that does happen, my model's effectiveness will decrease, hurting my chances at winning contests.  

****

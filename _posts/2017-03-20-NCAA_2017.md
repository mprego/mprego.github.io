---
layout: post
title: NCAA Tournament 2017 Model
---

Take two on building a model to produce a decent bracket for the 2017 NCAA Men's Basketball Tournament.

The madness of March is here again, and I'm once again trying to figure out this madness with a sound statistical model for the 2017 NCAA Men's Basketball Tournament.  Despite my bracket's terrible 15th percentile performance from last year, I'm still determined that a model can exceed in the bracket competition.  The biggest reason for why a model can succeed is the plethora of data points.  This tournament (and the accompanying regular seasons) has been going on for years and has been well documented in terms of stats and metrics.  With so many variables and trials, it's only a matter of time before a model can adequately predict the outcome of tournament games.  The biggest challenge to the model is the unforgiving nature single elimination nature of the NCAA tournament, where a single game can break brackets.  Due to this randomness, the model may have confusing data to train and learn upon, potentially resulting in weird predictions and outcomes.  To succeed, the model will have to discern signal from noise and upset from fluke.  

In this project, I go through the following steps to produce a bracket for the 2017 tournament:

1. Collect data
2. Explore variables
3. Explore different types of models
4. Show results of 2017 NCAA model

### Collect data
All the data for this model comes from Kaggle's 2017 NCAA Tournament competition.  The dataset includes regular season and tournament data from 1985 to 2016.  The data includes basic box score metrics, such as shot attempts, rebounds, assists, steals, etc.  Unfortunately, the data does not contain any advanced metrics or rankings, such as RPI, BPI, and AP rankings.

### Explore variables
To determine which simple and derived variables, I first brainstormed which factors would have the biggest impact on tournament outcomes.  The list includes:

- Pace (Number of Possessions and Consistency of this Pace)
- Offensive and Defensive Efficiency per Possession
- Winning Percentage and Strength of Regular Season Schedule
- Success on the Road
- 3 Point Offensive and Defense
- Close Game Performance

Using the regular season data from Kaggle, I created metrics that are proxies to each of these features.  The exact variable definitions as well as their distribution can be found [here] (https://github.com/mprego/NCAA_2017/blob/master/notebooks/Variable%20Exploration%20V1.ipynb).  Next, I looked at how the relationship between these variables and the margin of victory for tournament games.  The full results can be found [here] (https://github.com/mprego/NCAA_2017/blob/master/notebooks/Variables%20vs%20Outcome%20V1.ipynb), but I'd like to point out a few variables with large correlations.

The next correlation belongs to strength of schedule (SOS).  Teams with harder strength of schedules generally have higher margins of victories.  Winning teams had a 0.34 correlation between their SOS and margin of victory, while losing teams have a 0.40 correlation between their SOS and margin of victory.  

The next biggest correlation belongs to winning percentage.  There was a 0.32 correlation for the winning team's winning percentage and a -0.34 correlation for the losing team's winning percentage.  

The next largest factor and maybe the most unintuitive is the team's performance in close games.  For this variable, I split a team's games into two categories: close games and not close games.  Close games were defined as those that went into OT or were decided by 5 points or less.  Using this partition of regular season games, I calculated the difference in winning percentage with this formula:

~~~~
Close Game Performance = (Winning Percentage of Close Games) - (Winning Percentage of Not Close Games)
~~~~

My hypothesis was that teams that performed well in these close games would also perform well in future close, competitive games, such as those in the tournament.  However, my hypothesis appears to be false.  There was a negative correlation between this metric and the margin of victory, suggesting that teams that did better in close regular season games did worse in tournament games.  My guess for this relationship is that a team's performance in close games is mostly luck, which does not carry over into the tournament.  In fact, this luck appears to slightly hurt performance in the tournament.  Perhaps the luck made teams overly confident or made the selection committee overly confident of the team.  For the winning team, the correlation was -0.14, and for the losing team, the correlation was 0.12.

Next, I decided to consolidate other variables in an effort to describe team playing styles.  My hypothesis was that each team has a preferred playing style and that each of these playing styles has advantages and disadvantages over other types of styles.  To figure out these playing styles, I used three variables, offensive efficiency, defensive efficiency, and pace variables, to form clusters using a K-Means model.  Here are the results of running the model with 2, 3, 4, and 5 clusters.

2 clusters

{% include 3d-scatter-k_means_2.html %}

3 clusters

{% include 3d-scatter-k_means_3.html %}

4 clusters

{% include 3d-scatter-k_means_4.html %}

5 clusters

{% include 3d-scatter-k_means_5.html %}

Based on these plots, I determined that 3 clusters made most sense.  (add more of an explanation).  When segmenting teams by these clusters, I found these differences:

| || Cluster 1 || Cluster 2 || Cluster 3 |
| Cluster 1 || 0.44 || -5.28 || -7.97 |
| Cluster 2 || 2.88 || -0.31 || -2.21 |
| Cluster 3 || 6.13 || 1.93 || 1.30 |

### Explore model types
Now that I've chosen my set of variables, I next decided which model to use.  My choices were:

- Ridge Regression
- Gradient Boosting Model
- Logistic Regression
- Support Vector Machine

Judging the performance of these models proved to be tricky as the first two models output numerical predictions for score differentials, while the second two models output classification predictions.  To make matters even more complex, my main objective wasn't either of these metrics.  My objective was to see which model produced the bracket with the most points.  This meant that not all games mattered equally, so taking the average of squared errors or the average of classification accuracies would not be sufficient.  I ended up making my own metric to evaluate success of these models:

~~~~
Tournament points = (correct 1st round outcomes)*2^0 + (correct 2nd round outcomes)*2^1 + (correct 3rd round outcomes)*2^2 + (correct 4th round outcomes)*2^3 + (correct 5th round outcomes)*2^4 + (correct 6th round outcomes)*2^5
~~~~

After running each of these models with the same output variables and cross-validating against their respective default scores (MSE for first two models and accuracy for last two), I obtained these scores:

| Model Type || Tournament Score |
| Ridge Regression || x |
| Gradient Boosting Model || y |
| Logistic Regression || w |
| Support Vector Machine || z |

With these results, I decided to use the (insert model) to make predictions for my bracket.

### Results of 2017 Bracket

### Next Steps
Separate models by round: some rounds have short turnaround
Maybe separate models by matchups in first round
Incorporate other rankings, such as RPI, BPI, and AP ranking
  may be hard due to lack of historical data



I figured it would also

In an ideal world, I would've considered a wide range of variables and time frames to use for my model.  Unfortunately, I was limited to the data I quickly scraped above.  I had the following variables for each team and each game:

1. Shooting: eFG%
2. Turnovers: TOV%
3. Rebounding: ORB%
4. Free Throws: FTFGA
5. Pace

The first four variables are the same Four Factors I used in my NBA model, but I felt that the college game has some important differences.  

First, the college game has many more teams.  The difference is so vast that there are more college conferences than NBA team: the NBA has 30 teams, while Division I college basketball has 32 conferences and 351 total teams.  In the NBA, each team plays every single other team at least once.  In college basketball, teams will play a majority of their games within their conference, playing a very small number of out of conference teams.

Second, the greater number of conferences, teams, and players leads to a much larger variance in talent.  Combined with the non-dispersive schedules mentioned above, team's schedules can vary in toughness.

To account for these differences, I also pulled an end of season SOS metric for each team from sports-reference.

For each game in the season, I calculated each team's average Four Factors and Pace of past games in the season.  Usually, I would've experimented by varying the number of past games to include, but since there are so few games in a season, I wanted to keep as much training data as possible.

### Model Type
I leveraged code from my [NBA modeling project](http://mprego.github.io/NBA_Model/), so I used the same two classification models: a Support Vector Machine (SVM) model and a Random Forest model.  

For each team in the tournament, I created a separate model.  The process looked like this:

- Get training data of regular season games, which contained opponent's metrics mentioned above and had a Win/Loss indicator as the outcome
- Feed training data to both types of models (SVM and Random Forest)
- Using cross validation, determine which model performs better
- Store best performing model for each team


### Simulation Method
To simulate the NCAA tournament, I imported the teams and their respective seedings.  Then for each matchup, I scored each team's model and extracted the win probability from each team's model.  I averaged the probabilities from both models in each matchup to come up with my outcome probabilities.  Using a random number generator mimicing this probability distribution, my simulation determined which team would advance.

Then with these win probability outcomes, I ran a Monte Carlo simulation of the tournament 100 times, tallying up which teams appeared most often in each round of the tournament.  Using that tallied up view, I filled out a bracket, keeping as true to the model as possible.

Keeping true to the model proved difficult sometimes due to [Simpson's paradox](https://en.wikipedia.org/wiki/Simpson%27s_paradox).  

![Simpson's Paradox]({{ site.baseurl }}/images/simpson.png)

In the case of the bracket, sometimes a team was the most likely team to be in a particular round, even though that wasn't the case for a previous round.  This phenomenon is caused by differing win probabilites at each round for this team.  The team has a lower win probability in the previous round, yet has a higher win probability in the later round.  When these situations arose, I used my best judgment to determine which scenario was more likely.  


### Results

Not so good.

In ESPN's Tournament Challenge, the bracket captured 500 points, putting the entry in the 15th percentile.  The model did get 3/8 for the Elite Eight, but failed pretty spectacularly in the next round.  Here's the round by round breakdown:

| Round of 64 || 240/320 |
| Round of 32 || 140/320 |
| Sweet 16 || 120/320 |
| Elite Eight || 0/320 |
| Final Four || 0/320 |
| Championship || 0/320 |

Here's the full bracket:

![Bracket]({{ site.baseurl }}/images/bracket.png)

### Improvements
Since I had such a short time frame to build the model and submit my predictions, I have a large list of items that I would've liked to do:

- Use past seasons and tournaments to truly cross validate my models and simulations
- After simulating the tournament, pick the outcome that generates the most amount of points according to a standard scoring system
- Use regression models instead of classification models because some teams rarely lose in a given season, leading to sparse outcomes
- Experiment with different time frames for the training dataset, such as using past seasons
- Instead of making a model for each team, make a model for a group of similar teams

****

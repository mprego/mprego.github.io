---
layout: post
title: NCAA Tournament 2017 Model
---

Take two on building a model to produce a decent bracket for the 2017 NCAA Men's Basketball Tournament.

The madness of March is here, and I'm once again trying to figure out this madness with a sound statistical model for the 2017 NCAA Men's Basketball Tournament.  Despite my bracket's terrible 15th percentile performance from last year, I'm still determined to make a model that excels in the bracket competition.  The biggest reason for why a model can succeed is the plethora of data points.  This tournament (and the accompanying regular seasons) has been going on for years and has been well documented in terms of stats and metrics.  With so many variables and trials, a model is well suited to predict the outcome of tournament games.  The biggest challenge to the model is the unforgiving single elimination nature of the NCAA tournament, where a single game can break brackets.  Due to this randomness, the model may have confusing data to train and learn upon, potentially resulting in weird predictions and outcomes.  To succeed, the model will have to discern signal from noise and upset from fluke.  

In this project, I go through the following steps to produce a bracket for the 2017 tournament:

1. Collect data
2. Explore variables
3. Explore different types of models
4. Show results of 2017 NCAA model

### Collect data
All the data for this model comes from Kaggle's 2017 NCAA Tournament competition.  The dataset includes regular season and tournament data from 1985 to 2016.  The data includes basic box score metrics, such as shot attempts, rebounds, assists, steals, etc.  Unfortunately, the data does not contain any advanced metrics or rankings, such as RPI, BPI, and AP rankings.

### Explore variables
To determine which simple and derived variables to use, I first brainstormed which factors would have the biggest impact on tournament outcomes.  The list included:

- Pace (Number of Possessions and Consistency of this Pace)
- Offensive and Defensive Efficiency per Possession
- Winning Percentage and Strength of Regular Season Schedule
- Success on the Road
- 3 Point Percentage on Offensive and Defense
- Close Game Performance

Using the regular season data from Kaggle, I created metrics that are proxies to each of these features.  The exact variable definitions as well as their distribution can be found [here](https://github.com/mprego/NCAA_2017/blob/master/notebooks/Variable%20Exploration%20V1.ipynb).  Next, I looked at the relationship between these variables and the margin of victory for tournament games.  The full results can be found [here](https://github.com/mprego/NCAA_2017/blob/master/notebooks/Variables%20vs%20Outcome%20V1.ipynb).  From this analysis, I'd like to point out a few variables with large correlations.

Strength of schedule (SOS) had a high correlation: teams with harder strength of schedules generally have higher margins of victories.  Winning teams had a 0.34 correlation between their SOS and margin of victory, while losing teams have a 0.40 correlation between their SOS and margin of victory.  

The next biggest correlation belongs to winning percentage.  There was a 0.32 correlation for the winning team's winning percentage and a -0.34 correlation for the losing team's winning percentage.  

The next biggest factor, and maybe the most unintuitive, is the team's performance in close games.  For this variable, I split a team's games into two categories: close games and not close games.  Close games were defined as those that went into OT or were decided by 5 points or less.  Using this partition of regular season games, I calculated the difference in winning percentage with this formula:

~~~~
Close Game Performance = (Winning Percentage of Close Games) - (Winning Percentage of Not Close Games)
~~~~

My hypothesis was that teams that performed well in these close games would also perform well in future close, competitive games, such as those in the tournament.  However, my hypothesis appears to be false.  There was a negative correlation between this metric and the margin of victory, suggesting that teams that did better in close regular season games did worse in tournament games.  My guess for this relationship is that a team's performance in close games is mostly luck, which does not carry over into the tournament.  In fact, this luck appears to slightly hurt performance in the tournament.  Perhaps the luck made teams overly confident or made the selection committee overly confident of the team.  For the winning team, the correlation was -0.14, and for the losing team, the correlation was 0.12.

Next, I decided to consolidate other variables in an effort to describe team playing styles.  My hypothesis was that each team has a preferred playing style and that each of these playing styles has advantages and disadvantages over other types of styles.  To figure out these playing styles, I used three variables, offensive efficiency, defensive efficiency, and pace variables, to form clusters using a K-Means model.  Here are the results of running the model with 2, 3, and 4 clusters.

2 Clusters

{% include 3d-scatter-k_means_2.html %}

3 Clusters

{% include 3d-scatter-k_means_3.html %}

4 Clusters

{% include 3d-scatter-k_means_4.html %}

Based on these plots, I determined that 3 clusters made most sense.  Based on the graphs, the 4 cluster model appeared to have too many groups, while the 2 cluster model seemed a bit too simplistic.  The 3 cluster model was a nice compromise between simplicity and having distinct groups.  When segmenting teams by this 3 cluster K-means model, I found these differences:

| || Cluster 1 || Cluster 2 || Cluster 3 |
| Cluster 1 || 0.44 ||  ||  |
| Cluster 2 || 4.08 || -0.31 ||  |
| Cluster 3 || 7.05 || 2.07 || 1.30 |

There are some large swings in point differential among different cluster matchups, making me feel comfortable with this selection of clusters.

### Explore model types
Now that I chose my set of variables, I next decided which model to use.  My choices were:

- Ridge Regression
- Gradient Boosting Model
- Logistic Regression
- Support Vector Machine

Judging the performance of these models proved to be tricky as the first two models output numerical predictions for score differentials, while the second two models output classification predictions.  To make matters even more complex, my main objective wasn't either of these metrics.  My objective was to see which model produced the bracket with the most points.  This meant that not all games mattered equally, so taking the average of squared errors or the average of classification accuracies would not be sufficient.  I ended up making my own metric to evaluate success of these models:

~~~~
Tournament points = (correct 1st round outcomes)*2^0 + (correct 2nd round outcomes)*2^1 + (correct 3rd round outcomes)*2^2 + (correct 4th round outcomes)*2^3 + (correct 5th round outcomes)*2^4 + (correct 6th round outcomes)*2^5
~~~~

After running each of these models with the same inputs variables and cross-validating against their respective default scores (MSE for first two models and accuracy for last two), I obtained the two best models:

| Model Type || Tournament Score |
| Ridge Regression || 82.5 |
| Logistic Regression || 86.2 |

With these results, I decided to use the logistic regression to make predictions for my bracket.

### Results of 2017 Bracket
The bracket did okay.  In ESPN's Bracket Challenge, it picked up 730 points (equivalent to 73 points in model) and was in the 59th percentile.  Looking at the round by round performance, the bracket started strong, but struggled immensely after the Sweet 16:

| Round of 64 || 250/320 |
| Round of 32 || 240/320 |
| Sweet 16 || 160/320 |
| Elite Eight || 80/320 |
| Final Four || 0/320 |
| Championship || 0/320 |

Here's the full bracket: ![Bracket]({{ site.baseurl }}/images/bracket.pd)

### Next Steps
Based on the results of this model and of last year's model, I think getting more data would have the most impactful improvement to my model.  With the current selection of data, I manipulated the data and tested several combinations of variables and model types.  Despite all of that testing, the results were not very different.  This parity suggests that the models are pretty much optimized given the data set.  

In terms of what other data would be useful, there are two categories: advanced statistics and human rankings.  For advanced statistics, I think having statistics incorporating player movement data would give the model a much better sense of a team's style of play, including team and player weaknesses and strengths.  For  human rankings such as AP polls, these rankings could help the model capture characteristics that we haven't figured out how to describe in data yet.  With both of these data sources, the model can weigh them against existing variables and hopefully improve model prediction power.


****
